#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨è”èµ›çˆ¬è™«è„šæœ¬
æ”¯æŒçˆ¬å–æ‡‚çƒå¸ä¸åŒè”èµ›çš„ç§¯åˆ†æ¦œæ•°æ®å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
"""

import requests
import re
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

from src.team_database import TeamDatabaseManager
from config.config import DONGQIUDI_CONFIG


class UniversalLeagueSpider:
    """
    é€šç”¨è”èµ›çˆ¬è™«ç±»
    æ”¯æŒçˆ¬å–ä¸åŒè”èµ›çš„ç§¯åˆ†æ¦œæ•°æ®
    """
    
    def __init__(self, league_name: str = "unknown"):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            league_name: è”èµ›åç§°
        """
        self.league_name = league_name
        self.logger = logging.getLogger(__name__)
        self.db_manager = TeamDatabaseManager()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = DONGQIUDI_CONFIG['headers'].copy()
        self.timeout = DONGQIUDI_CONFIG['timeout']
        
    def extract_standing_data(self, url: str) -> Optional[str]:
        """
        ä»æŒ‡å®šURLæå–ç§¯åˆ†æ¦œæ•°æ®
        
        Args:
            url: è”èµ›æ•°æ®é¡µé¢URL
            
        Returns:
            æå–åˆ°çš„åŸå§‹æ•°æ®å­—ç¬¦ä¸²ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            self.logger.info(f"æ­£åœ¨è¯·æ±‚: {url}")
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            response.raise_for_status()
            
            self.logger.info(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            self.logger.info(f"å“åº”å†…å®¹é•¿åº¦: {len(response.text)}")
            
            # å¤šç§æœç´¢æ¨¡å¼ï¼Œæé«˜æ•°æ®æå–æˆåŠŸç‡
            standing_patterns = [
                # æ¨¡å¼1: æœç´¢åŒ…å«ç§¯åˆ†æ¦œå…³é”®å­—æ®µçš„å®Œæ•´æ•°æ®å—
                r'data:\s*\[\{[^\]]*(?:desc|goals_against|goals_pro|matches_draw|matches_lost|matches_total|matches_won|points|rank|team_id|team_name)[^\]]*\}\]',
                
                # æ¨¡å¼2: æœç´¢åŒ…å«team_nameå’Œå…¶ä»–å…³é”®å­—æ®µçš„æ•°æ®
                r'\[\{[^\]]*team_name[^\]]*team_id[^\]]*team_logo[^\]]*scheme[^\]]*\}[^\]]*\]',
                
                # æ¨¡å¼3: æ›´å®½æ³›çš„æœç´¢ï¼Œå¯»æ‰¾åŒ…å«å¤šä¸ªçƒé˜Ÿæ•°æ®çš„æ•°ç»„
                r'\[(?:\{[^\}]*team_name[^\}]*\}[,\s]*){3,}\]',
                
                # æ¨¡å¼4: æœç´¢JavaScriptå˜é‡èµ‹å€¼ä¸­çš„æ•°æ®
                r'(?:data|teams|standing)\s*[:=]\s*\[\{[^\]]*team_name[^\]]*\}[^\]]*\]',
                
                # æ¨¡å¼5: æœç´¢åŒ…å«ç§¯åˆ†æ¦œæ ¸å¿ƒå­—æ®µçš„åŒºåŸŸ
                r'(?=.*team_name)(?=.*team_id)(?=.*points)(?=.*rank).{100,3000}',
            ]
            
            extracted_data = None
            for i, pattern in enumerate(standing_patterns):
                self.logger.info(f"å°è¯•æ¨¡å¼ {i+1}: {pattern[:50]}...")
                matches = re.findall(pattern, response.text, re.DOTALL | re.IGNORECASE)
                
                if matches:
                    self.logger.info(f"âœ… æ¨¡å¼ {i+1} æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
                    # é€‰æ‹©æœ€é•¿çš„åŒ¹é…ä½œä¸ºæœ€å¯èƒ½çš„æ•°æ®
                    extracted_data = max(matches, key=len)
                    self.logger.info(f"é€‰æ‹©æ•°æ®é•¿åº¦: {len(extracted_data)}")
                    break
                else:
                    self.logger.info(f"âŒ æ¨¡å¼ {i+1} æœªæ‰¾åˆ°åŒ¹é…")
            
            if extracted_data:
                self.logger.info(f"æˆåŠŸæå–æ•°æ®ï¼Œé•¿åº¦: {len(extracted_data)}")
                return extracted_data
            else:
                self.logger.warning("æ‰€æœ‰æ¨¡å¼éƒ½æœªèƒ½æå–åˆ°æ•°æ®")
                return None
                
        except requests.RequestException as e:
            self.logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")
            return None
        except Exception as e:
            self.logger.error(f"æå–æ•°æ®å¼‚å¸¸: {e}")
            return None
    
    def parse_javascript_object(self, js_data: str) -> Optional[List[Dict[str, Any]]]:
        """
        è§£æJavaScriptå¯¹è±¡æ ¼å¼çš„æ•°æ®
        
        Args:
            js_data: JavaScriptå¯¹è±¡å­—ç¬¦ä¸²
            
        Returns:
            è§£æåçš„æ•°æ®åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            # æ¸…ç†æ•°æ®ï¼Œç§»é™¤å¯èƒ½çš„å‰ç¼€
            cleaned_data = js_data.strip()
            if ':' in cleaned_data and cleaned_data.index(':') < 50:
                cleaned_data = cleaned_data[cleaned_data.index('['):]
            
            # å°è¯•ç›´æ¥è§£æJSON
            try:
                parsed_data = json.loads(cleaned_data)
                if isinstance(parsed_data, list) and len(parsed_data) > 0:
                    self.logger.info(f"ç›´æ¥JSONè§£ææˆåŠŸï¼Œè·å¾— {len(parsed_data)} æ¡è®°å½•")
                    return parsed_data
            except json.JSONDecodeError:
                pass
            
            # JavaScriptå¯¹è±¡è½¬JSONçš„è½¬æ¢è§„åˆ™
            conversions = [
                # ä¸ºæœªåŠ å¼•å·çš„é”®åæ·»åŠ å¼•å·
                (r'\b([a-zA-Z_$][a-zA-Z0-9_$]*)\s*:', r'"\1":'),
                # ä¸ºæœªåŠ å¼•å·çš„å­—ç¬¦ä¸²å€¼æ·»åŠ å¼•å·ï¼ˆæ’é™¤æ•°å­—ã€å¸ƒå°”å€¼ã€nullï¼‰
                (r':\s*([a-zA-Z_$][a-zA-Z0-9_$:/\.\-]*?)\s*([,}])', r': "\1"\2'),
                # å¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚$ç¬¦å·ï¼‰
                (r'([a-zA-Z0-9_])\$([a-zA-Z0-9_])', r'\1_DOLLAR_\2'),
                # å¤„ç†å•å¼•å·
                (r"'", r'"'),
            ]
            
            converted_data = cleaned_data
            for pattern, replacement in conversions:
                converted_data = re.sub(pattern, replacement, converted_data)
            
            # å†æ¬¡å°è¯•è§£æ
            try:
                parsed_data = json.loads(converted_data)
                if isinstance(parsed_data, list) and len(parsed_data) > 0:
                    self.logger.info(f"è½¬æ¢åJSONè§£ææˆåŠŸï¼Œè·å¾— {len(parsed_data)} æ¡è®°å½•")
                    return parsed_data
            except json.JSONDecodeError as e:
                self.logger.warning(f"è½¬æ¢åä»æ— æ³•è§£æJSON: {e}")
            
            # æ‰‹åŠ¨è§£æJavaScriptå¯¹è±¡æ ¼å¼
            return self._manual_parse_js_object(cleaned_data)
            
        except Exception as e:
            self.logger.error(f"è§£æJavaScriptå¯¹è±¡å¼‚å¸¸: {e}")
            return None
    
    def _manual_parse_js_object(self, js_data: str) -> Optional[List[Dict[str, Any]]]:
        """
        æ‰‹åŠ¨è§£æJavaScriptå¯¹è±¡æ ¼å¼çš„æ•°æ®
        
        Args:
            js_data: JavaScriptå¯¹è±¡å­—ç¬¦ä¸²
            
        Returns:
            è§£æåçš„æ•°æ®åˆ—è¡¨
        """
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¯¹è±¡
            object_pattern = r'\{([^{}]*(?:\{[^{}]*\}[^{}]*)*)\}'
            objects = re.findall(object_pattern, js_data)
            
            parsed_objects = []
            for obj_content in objects:
                obj_dict = {}
                
                # è§£æé”®å€¼å¯¹
                kv_pattern = r'([a-zA-Z_$][a-zA-Z0-9_$]*)\s*:\s*([^,}]+)'
                matches = re.findall(kv_pattern, obj_content)
                
                for key, value in matches:
                    # æ¸…ç†å€¼
                    value = value.strip().strip('"\'')
                    obj_dict[key] = value
                
                # åªä¿ç•™åŒ…å«å¿…è¦å­—æ®µçš„å¯¹è±¡
                if 'team_name' in obj_dict or 'team_id' in obj_dict:
                    parsed_objects.append(obj_dict)
            
            if parsed_objects:
                self.logger.info(f"æ‰‹åŠ¨è§£ææˆåŠŸï¼Œè·å¾— {len(parsed_objects)} æ¡è®°å½•")
                return parsed_objects
            else:
                self.logger.warning("æ‰‹åŠ¨è§£ææœªæ‰¾åˆ°æœ‰æ•ˆå¯¹è±¡")
                return None
                
        except Exception as e:
            self.logger.error(f"æ‰‹åŠ¨è§£æå¼‚å¸¸: {e}")
            return None
    
    def extract_team_info(self, teams_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä»è§£æçš„æ•°æ®ä¸­æå–çƒé˜Ÿä¿¡æ¯
        
        Args:
            teams_data: è§£æåçš„çƒé˜Ÿæ•°æ®åˆ—è¡¨
            
        Returns:
            æå–çš„çƒé˜Ÿä¿¡æ¯åˆ—è¡¨
        """
        extracted_teams = []
        
        for team_data in teams_data:
            try:
                # æå–å¿…éœ€çš„å­—æ®µ
                team_info = {
                    'team_name': team_data.get('team_name', ''),
                    'team_id': team_data.get('team_id', ''),
                    'team_logo': team_data.get('team_logo', ''),
                    'scheme': team_data.get('scheme', ''),
                    'league': self.league_name,
                    'extracted_at': datetime.now().isoformat()
                }
                
                # éªŒè¯å¿…éœ€å­—æ®µ
                if all([team_info['team_name'], team_info['team_id']]):
                    extracted_teams.append(team_info)
                    self.logger.debug(f"æå–çƒé˜Ÿ: {team_info['team_name']} (ID: {team_info['team_id']})")
                else:
                    self.logger.warning(f"çƒé˜Ÿæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡: {team_data}")
                    
            except Exception as e:
                self.logger.error(f"æå–çƒé˜Ÿä¿¡æ¯å¼‚å¸¸: {e}")
                continue
        
        self.logger.info(f"æˆåŠŸæå– {len(extracted_teams)} æ”¯çƒé˜Ÿä¿¡æ¯")
        return extracted_teams
    
    def save_teams_to_database(self, teams_info: List[Dict[str, Any]]) -> int:
        """
        å°†çƒé˜Ÿä¿¡æ¯ä¿å­˜åˆ°æ•°æ®åº“
        
        Args:
            teams_info: çƒé˜Ÿä¿¡æ¯åˆ—è¡¨
            
        Returns:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        try:
            # è¿æ¥æ•°æ®åº“
            if not self.db_manager.connect():
                self.logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥")
                return 0
            
            # æ‰¹é‡æ’å…¥
            success_count = self.db_manager.insert_teams_batch(teams_info)
            
            self.logger.info(f"æ•°æ®åº“ä¿å­˜å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{len(teams_info)}")
            return success_count
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜åˆ°æ•°æ®åº“å¼‚å¸¸: {e}")
            return 0
        finally:
            self.db_manager.close()
    
    def crawl_league(self, url: str) -> Tuple[bool, int]:
        """
        çˆ¬å–æŒ‡å®šè”èµ›çš„æ•°æ®
        
        Args:
            url: è”èµ›æ•°æ®é¡µé¢URL
            
        Returns:
            Tuple[bool, int]: (æ˜¯å¦æˆåŠŸ, ä¿å­˜çš„çƒé˜Ÿæ•°é‡)
        """
        try:
            self.logger.info(f"å¼€å§‹çˆ¬å–è”èµ›: {self.league_name}")
            self.logger.info(f"ç›®æ ‡URL: {url}")
            
            # 1. æå–åŸå§‹æ•°æ®
            raw_data = self.extract_standing_data(url)
            if not raw_data:
                self.logger.error("æœªèƒ½æå–åˆ°åŸå§‹æ•°æ®")
                return False, 0
            
            # 2. è§£ææ•°æ®
            parsed_data = self.parse_javascript_object(raw_data)
            if not parsed_data:
                self.logger.error("æœªèƒ½è§£ææ•°æ®")
                return False, 0
            
            # 3. æå–çƒé˜Ÿä¿¡æ¯
            teams_info = self.extract_team_info(parsed_data)
            if not teams_info:
                self.logger.error("æœªèƒ½æå–åˆ°çƒé˜Ÿä¿¡æ¯")
                return False, 0
            
            # 4. ä¿å­˜åˆ°æ•°æ®åº“
            saved_count = self.save_teams_to_database(teams_info)
            
            if saved_count > 0:
                self.logger.info(f"âœ… è”èµ› {self.league_name} çˆ¬å–æˆåŠŸï¼Œä¿å­˜ {saved_count} æ”¯çƒé˜Ÿ")
                return True, saved_count
            else:
                self.logger.error(f"âŒ è”èµ› {self.league_name} æ•°æ®ä¿å­˜å¤±è´¥")
                return False, 0
                
        except Exception as e:
            self.logger.error(f"çˆ¬å–è”èµ›å¼‚å¸¸: {e}")
            return False, 0


def crawl_premier_league() -> Tuple[bool, int]:
    """
    çˆ¬å–è‹±è¶…è”èµ›æ•°æ®
    
    Returns:
        Tuple[bool, int]: (æ˜¯å¦æˆåŠŸ, ä¿å­˜çš„çƒé˜Ÿæ•°é‡)
    """
    spider = UniversalLeagueSpider("è‹±è¶…")
    return spider.crawl_league("https://www.dongqiudi.com/data/1")


def crawl_csl() -> Tuple[bool, int]:
    """
    çˆ¬å–ä¸­è¶…è”èµ›æ•°æ®
    
    Returns:
        Tuple[bool, int]: (æ˜¯å¦æˆåŠŸ, ä¿å­˜çš„çƒé˜Ÿæ•°é‡)
    """
    spider = UniversalLeagueSpider("ä¸­è¶…")
    return spider.crawl_league("https://www.dongqiudi.com/data/231")


def crawl_custom_league(league_name: str, url: str) -> Tuple[bool, int]:
    """
    çˆ¬å–è‡ªå®šä¹‰è”èµ›æ•°æ®
    
    Args:
        league_name: è”èµ›åç§°
        url: è”èµ›æ•°æ®é¡µé¢URL
        
    Returns:
        Tuple[bool, int]: (æ˜¯å¦æˆåŠŸ, ä¿å­˜çš„çƒé˜Ÿæ•°é‡)
    """
    spider = UniversalLeagueSpider(league_name)
    return spider.crawl_league(url)


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ç¤ºä¾‹ï¼šçˆ¬å–è‹±è¶…æ•°æ®
    print("ğŸš€ å¼€å§‹çˆ¬å–è‹±è¶…æ•°æ®...")
    success, count = crawl_premier_league()
    
    if success:
        print(f"âœ… è‹±è¶…æ•°æ®çˆ¬å–æˆåŠŸï¼Œä¿å­˜äº† {count} æ”¯çƒé˜Ÿ")
    else:
        print("âŒ è‹±è¶…æ•°æ®çˆ¬å–å¤±è´¥")